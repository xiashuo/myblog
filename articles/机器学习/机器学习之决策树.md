<div class="blog-article">
    <h1><a href="p.html?p=机器学习/机器学习之决策树" class="title">机器学习之决策树</a></h1>
    <span class="author">xiashuobad</span>
    <span class="time">2017-09-20 14:38</span>
    <span><a href="tags.html?t=机器学习" class="tag">机器学习</a></span>
    </div>
<br/>

　　上一篇文章的k-近邻算法可以完成很多分类任务，但是它最大的缺点就是无法给出数据的内在含义，决策树的主要优势就在于数据形式非常容易理解。决策树的一个重要任务是为了数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，在这些机器根据数据集创建规则时，就是机器学习的过程。
## 决策树的构造 ##
　　在构造决策树时，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则当前无需阅读的垃圾邮件已经正确地划分数据分类，
无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程。如何划分数据子集的算法和划分原始数据集的方法相同，直到所有具有相同类型的数据均在一个数据子集内。

　　创建分支的伪代码函数createBranch()如下所示：

	检测数据集中的每个子项是否属于同一分类：
		If so return 类标签；
		Else
		寻找划分数据集的最好特征
		划分数据集
		创建分支节点
			for 每个划分的子集
				调用函数createBranch并增加返回结果到分支节点中
		return 分支节点

　　上面的伪代码createBranch是一个递归函数，在倒数第二行直接调用了它自己。后面我们将把上面的伪代码转换为Python代码，这里我们需要进一步了解算法是如何划分数据集的。
### 决策树的一般流程： ###
1. 收集数据：可以使用任何方法。
2. 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。
3. 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。
4. 训练算法：构造树的数据结构。
5. 测试算法：使用经验树计算错误率。
6. 使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。

### 信息增益 ###
　　划分数据集的大原则是：将无序的数据变得更加有序。我们可以使用多种方法划分数据集，但是每种方法都有各自的优缺点。组织杂乱无章数据的一种方法就是使用信息论度量信息， 信息论是量化处理信息的分支科学。我们可以在划分数据之前或之后使用信息论量化度量信息的内容。

　　**在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。**

　　在可以评测哪种数据划分方式是最好的数据划分之前，我们必须学习如何计算信息增益。集合信息的度量方式称为**香农熵**或者简称为**熵**，这个名字来源于信息论之父克劳德·香农。

　　熵定义为信息的期望值，在明晰这个概念之前，我们必须知道信息的定义。如果待分类的事务可能划分在多个分类之中，则符号的信息定义为：

　　　　　　　　　　　　　　　![](assets/images/2017/09/ppb3nqa2jcg0irqpuaris580gn.png)

其中p(xi)是选择该分类的概率。

　　为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面的公式得到：

　　　　　　　　　　　　　　　　　![](assets/images/2017/09/qaoredokf4ip8rnca2becr32cp.png)

其中n是分类的数目。
### 计算熵的程序 ###
	from math import log
	def calcShannonEnt(dataSet):
	    numEntries = len(dataSet) #计算数据集中实例的总数
	    labelCounts = {}   #用来记录每个标签出现的次数
	    for featVec in dataSet: 
	        currentLabel = featVec[-1] #每个实例最后一个特征元素为其分类标签
	        #如果当前标签在labelCounts.keys()中存在，则该标签计数加1
	        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0
	        labelCounts[currentLabel] += 1
	    shannonEnt = 0.0
	    for key in labelCounts:
	        prob = float(labelCounts[key])/numEntries #这个是每个标签出现的概率，相当于公式里的P(Xi)
	        shannonEnt -= prob * log(prob,2) #利用了上面的熵计算公式
	    return shannonEnt

　　我们可以利用createDataSet()函数得到一个简单的数据集，函数如下：

	def createDataSet():
	    dataSet = [[1, 1, 'yes'],
	               [1, 1, 'yes'],
	               [1, 0, 'no'],
	               [0, 1, 'no'],
	               [0, 1, 'no']]
	    labels = ['no surfacing','flippers']
	    #change to discrete values
	    return dataSet, labels

**运行下面代码：**

	myDat, labels = createDataSet()
	print(myDat)
	print(calcShannonEnt(myDat))

**结果：**

	[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]
	
	0.9709505944546686

　　得到熵之后，我们就可以按照获取最大信息增益的方法划分数据集。

### 划分数据集 ###
　　分类算法除了需要测量信息熵，还需要划分数
据集，度量划分数据集的熵，以便判断当前是否正确地划分了数据集。

#### 按照给定特征划分数据集 ####

	def splitDataSet(dataSet, axis, value):#dataSet为要划分的数据集，axis是以哪个特征划分，value是该特征的值
	    retDataSet = []
	    for featVec in dataSet:
	        if featVec[axis] == value:
	            reducedFeatVec = featVec[:axis]  #把在axis左边的元素放在列表里
	            reducedFeatVec.extend(featVec[axis+1:])#把在axis右边的元素放在列表里
	            retDataSet.append(reducedFeatVec) #将上面修改后的列表加入到划分数据集中
	    return retDataSet
#### 相关函数学习　
apend()函数：

假定存在两个列表， a和b：	

    In [1]: a=[1,2,3]
    In [2]: b=[4,5,6]
    In [3]: a.append(b)
    In [4]: a
    Out[4]: [1, 2, 3, [4, 5, 6]]				
　　如果执行a.append(b)，则列表得到了第四个元素，而且第四个元素也是一个列表。然而
如果使用extend方法：

	In [5]: a=[1,2,3]
	In [6]: a.extend(b)
	In [7]: a
	Out[7]: [1, 2, 3, 4, 5, 6]
则得到一个包含a和b所有元素的列表。

**运行下面代码测试splitDataSet函数：**

	myDat, labels = createDataSet()
    print(myDat)
    print(splitDataSet(myDat,0,0))
    print(splitDataSet(myDat,0,1))
    print(splitDataSet(myDat,1,0))
    print(splitDataSet(myDat,1,1))
结果：

	[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]
	[[1, 'no'], [1, 'no']]
	[[1, 'yes'], [1, 'yes'], [0, 'no']]
	[[1, 'no']]
	[[1, 'yes'], [1, 'yes'], [0, 'no'], [0, 'no']]

这里一共有四种划分方式，我们需要找到最好的划分方式，这里用的方法是遍历整个数据集，计算每种划分后的信息熵，信息增益(即熵的减少)最大的即为最好的划分。
### 选择最好的数据集划分方式 ###
	def chooseBestFeatureToSplit(dataSet):
	    numFeatures = len(dataSet[0]) - 1      #每行最后一列是标签，故减1
	    baseEntropy = calcShannonEnt(dataSet) #计算初始熵
	    bestInfoGain = 0.0; bestFeature = -1
	    for i in range(numFeatures):        #遍历所有的特征划分
	        featList = [example[i] for example in dataSet]#将每行数据的第i列存入列表中
	        # print(featList)
	        uniqueVals = set(featList)   #列表会有相同的数据，使用set去掉重复的
	        # print(uniqueVals)
	        newEntropy = 0.0
	        for value in uniqueVals:
	            subDataSet = splitDataSet(dataSet, i, value) #利用第i个特征进行划分，并对每个唯一特征值求得的熵求和
	            prob = len(subDataSet)/float(len(dataSet))
	            newEntropy += prob * calcShannonEnt(subDataSet)
	        infoGain = baseEntropy - newEntropy     #计算信息增益
	        if (infoGain > bestInfoGain):       
	            bestInfoGain = infoGain         #选信息增益最大的特征的索引，并返回
	            bestFeature = i
	    return bestFeature
**运行下面程序测试：**

	myDat, labels = createDataSet()
    print(myDat)
    print(chooseBestFeatureToSplit(myDat))
**结果：**

	[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]
	0
结果显示，以第0个特征进行划分是最好的。
### 递归构建决策树 ###
　　上面写了这么多，都是在为构建决策树做准备，也就是构建决策树需要的子模块，其工作原理如下：**得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于
两个分支的数据集划分。第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节
点上，我们可以再次划分数据。**因此我们可以采用递归的原则处理数据集。

　　**递归结束的条件**是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有
相同的分类。

　　**一个需要考虑的问题**是，如果数据集已经处理了所有属性，但是类标签依然不是唯一
的，此时我们需要决定如何定义该叶子节点，在这种情况下，我们通常会采用多数表决的方法决
定该叶子节点的分类。
#### 多数表决方法的程序 ####
	def majorityCnt(classList):
	    classCount={}
	    for vote in classList:
	        # if vote not in classCount.keys(): classCount[vote] = 0
	        # classCount[vote] += 1
	        classCount[vote]=classCount.get(vote,0)+1 
	    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)
	    return sortedClassCount[0][0]
　　这个与上一篇文章里ＫＮＮ算法里的knn_classify函数里的投票表决代码类似，该函数使用分类名称的列表，然后创建键值为classList中唯一值的数据字典，字典对象存储了classList中每个类标签出现的频率，最后利用operator操作键值排序字典，并返回出现次数最多的分类名称。
#### 创建树的函数代码 ####

	def createTree(dataSet,labels):
	    classList = [example[-1] for example in dataSet] #获取数据集的所有类标签
	    if classList.count(classList[0]) == len(classList): #如果所有类标签完全相同，则返回该类标签，递归结束
	        return classList[0]
	    if len(dataSet[0]) == 1: #如果所有的特征已经使用完了，仍然没有是数据集划分成仅包含唯一类别的分组，
	        return majorityCnt(classList)#则返回出现次数最多的标签
	    bestFeat = chooseBestFeatureToSplit(dataSet)#选择最好的划分特征，这里返回的是最好特征的索引值
	    bestFeatLabel = labels[bestFeat] #最好特征的值
	    myTree = {bestFeatLabel:{}} #初始化树结构
	    del(labels[bestFeat]) #删除这个特征
	    featValues = [example[bestFeat] for example in dataSet] #获取该特征的所有取值
	    uniqueVals = set(featValues) #用set函数，去掉相同重复的
	    for value in uniqueVals:
	        subLabels = labels[:] #在Python语言中函数参数是列表类型时，参数是按照引用方式传递的，为了保证每次调用函数createTree()时不改变原始列表的内容，使用新变量subLabels代替原始列表。
	        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)#进行递归构建，重复上面的过程
	    return myTree
**测试代码：**
	
	myDat,labels=createDataSet()
	myTree=createTree(myDat, labels)
	print(myTree)
**结果：**

	{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}
　　变量myTree包含了很多代表树结构信息的嵌套字典，从左边开始，第一个关键字nosurfacing是第一个划分数据集的特征名称，该关键字的值也是另一个数据字典。第二个关键字是no surfacing特征划分的数据集，这些关键字的值是no surfacing节点的子节点。这些值可能是类标签，也可能是另一个数据字典。如果值是类标签，则该子节点是叶子节点；如果值是另一个数据字典，则子节点是一个判断节点，这种格式结构不断重复就构成了整棵树。
## 在 Python 中使用 Matplotlib 注解绘制树形图 ##
　　Matplotlib提供了一个注解工具annotations，非常有用，它可以在数据图形上添加文本注
释。注解通常用于解释数据的内容。由于数据上面直接存在文本描述非常丑陋，因此工具内嵌支
持带箭头的划线工具，使得我们可以在其他恰当的地方指向数据位置，并在此处添加描述信息，
解释数据内容。
### 构造注解树 ###
　　绘制一棵完整的树需要一些技巧。我们虽然有x、 y坐标，但是如何放置所有的树节点却是个问题。我们必须知道有多少个叶节点，以便可以正确确定x轴的长度；我们还需要知道树有多少层，以便可以正确确定y轴的高度。这里我们定义两个新函数getNumLeafs()和getTreeDepth()，来获取叶节点的数目和树的层数。
#### 获取叶节点的数目和树的层数 ####
	＃获取叶子节点数目
	def getNumLeafs(myTree):
	    numLeafs = 0
	    firstStr = list(myTree)[0]
	    secondDict = myTree[firstStr]
	    for key in secondDict.keys():
	        if type(secondDict[key]).__name__ == 'dict':  # 判断节点值的类型是否为字典
	            numLeafs += getNumLeafs(secondDict[key]) #如果是，则进行递归，进入下个节点
	        else:       #如果不是，则该节点为叶子节点。
	            numLeafs += 1
	    return numLeafs
	//获取树的深度
	def getTreeDepth(myTree):
    maxDepth = 0
    firstStr = list(myTree)[0]
    secondDict = myTree[firstStr]
    for key in secondDict.keys():
        if type(secondDict[
                    key]).__name__ == 'dict': #判断节点值的类型是否为字典
            thisDepth = 1 + getTreeDepth(secondDict[key]) #如果是，则当前深度加1，并递归进入下个节点。
        else:
            thisDepth = 1  #如果是，则当前深度为1
        if thisDepth > maxDepth: maxDepth = thisDepth 
    return maxDepth #循环完毕，返回最大深度值
　　为了方便测试，创建函数retrieveTree输出预先存储的树信息，避免了每次测试代码时都要从数据中创建树的麻烦。

	def retrieveTree(i):
	    listOfTrees = [{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}},
	                   {'no surfacing': {0: 'no', 1: {'flippers': {0: {'head': {0: 'no', 1: 'yes'}}, 1: 'no'}}}}
	                   ]
	    return listOfTrees[i]

**测试：**

	myTree = retrieveTree(0)
    print(getNumLeafs(myTree))
    print(getTreeDepth(myTree))
**结果：**

	3
	2
　　函数retrieveTree()主要用于测试，返回预定义的树结构。上述命令中调用getNumLeafs()
函数返回值为3，等于树0的叶子节点数；调用getTreeDepths()函数也能够正确返回树的层数。

　　现在我们可以将前面的方法组合在一起，绘制一棵完整的树。

	# _*_ coding:utf-8 _*_
	import matplotlib.pyplot as plt
	# 定义决策树决策结果的属性，用字典来定义
	# 下面的字典定义也可写作 decisionNode={boxstyle:'sawtooth',fc:'0.8'}
	# boxstyle为文本框的类型，sawtooth是锯齿形，fc是边框线粗细
	decisionNode = dict(boxstyle="sawtooth", fc="0.8")
	leafNode = dict(boxstyle="round4", fc="0.8")
	arrow_args = dict(arrowstyle="<-")
	
	def createPlot(inTree):
	    fig = plt.figure(1, facecolor='white') # 定义一个画布，背景为白色
	    fig.clf() # 把画布清空
	    axprops = dict(xticks=[], yticks=[])# 定义横纵坐标轴，无内容
	    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)# 绘制图像,无边框,无坐标轴
	    plotTree.totalW = float(getNumLeafs(inTree))  #全局变量宽度 = 叶子数
	    plotTree.totalD = float(getTreeDepth(inTree)) #全局变量高度 = 深度
	    plotTree.xOff = -0.5 / plotTree.totalW; #例如绘制3个叶子结点，坐标应为1/3,2/3,3/3，为了显示效果，将x向左移一点（我只能这么理解了）
	    plotTree.yOff = 1.0;
	    plotTree(inTree, (0.5, 1.0), '') 
	    plt.show()
	＃绘制节点
	def plotNode(nodeTxt, centerPt, parentPt, nodeType):
	    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords='axes fraction',
	                            xytext=centerPt, textcoords='axes fraction',
	                            va="center", ha="center", bbox=nodeType, arrowprops=arrow_args)	
	#在两个节点中间标注
	def plotMidText(cntrPt, parentPt, txtString):
	    xMid = (parentPt[0] - cntrPt[0]) / 2.0 + cntrPt[0]
	    yMid = (parentPt[1] - cntrPt[1]) / 2.0 + cntrPt[1]
	    createPlot.ax1.text(xMid, yMid, txtString, va="center", ha="center", rotation=30)
	
	def plotTree(myTree, parentPt, nodeTxt):  # if the first key tells you what feat was split on
	    numLeafs = getNumLeafs(myTree)  # 当前树的叶子节点数目
	    depth = getTreeDepth(myTree) #这里好像并没有用到
	    firstStr = list(myTree)[0]  # 当前第一个节点
	    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs)) / 2.0 / plotTree.totalW, plotTree.yOff)#确定当前节点的绘制位置
	    plotMidText(cntrPt, parentPt, nodeTxt) #标注两个节点之间的信息
	    plotNode(firstStr, cntrPt, parentPt, decisionNode)#绘制决策节点
	    secondDict = myTree[firstStr]
	    plotTree.yOff = plotTree.yOff - 1.0 / plotTree.totalD
	    for key in secondDict.keys():
	        if type(secondDict[
	                    key]).__name__ == 'dict':  # test to see if the nodes are dictonaires, if not they are leaf nodes
	            plotTree(secondDict[key], cntrPt, str(key))  # recursion
	        else:  # it's a leaf node print the leaf node
	            plotTree.xOff = plotTree.xOff + 1.0 / plotTree.totalW
	            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)
	            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))
	    plotTree.yOff = plotTree.yOff + 1.0 / plotTree.totalD
**测试：**

	myTree=retrieveTree(0)
    createPlot(myTree)
**结果：**
	
	{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}

![](assets/images/2017/09/3igmgo52jajilojht6ql4qjkqk.png)

## 使用决策树执行分类 ##
　　依靠训练数据构造了决策树之后，我们可以将它用于实际数据的分类。在执行数据分类时，
需要决策树以及用于构造树的标签向量。然后，程序比较测试数据与决策树上的数值，递归执行
该过程直到进入叶子节点；最后将测试数据定义为叶子节点所属的类型。

	def classify(inputTree,featLabels,testVec):
	    firstStr = list(inputTree)[0] #当前节点的标签名称
	    secondDict = inputTree[firstStr] #子树
	    featIndex = featLabels.index(firstStr)# 将标签转换为索引
	    key = testVec[featIndex] #当前的特征标签
	    valueOfFeat = secondDict[key]
	    if isinstance(valueOfFeat, dict):#判断是否为字典类型
	        classLabel = classify(valueOfFeat, featLabels, testVec) #如果是，则继续递归分类
	    else: classLabel = valueOfFeat #否则，返回当前标签
	    return classLabel
**测试：**

	myDat,labels=createDataSet()
    featLabels=deepcopy(labels)＃这里需要使用到深克隆，复制labels的值，因为下面的createTree函数中会改变参数labels的值。
    myTree=createTree(myDat, labels)
    print(myTree)
	print(classify(myTree,featLabels,[1,1]))
    print(classify(myTree,featLabels,[0,0]))
**结果：**

	{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}
	yes
	no
**结果没毛病**
## 示例：使用决策树预测隐形眼镜类型 ##
　　使用小数据集，我们就可以利用决策树学到很多知识：眼科医生是如何判断患者需要佩戴的镜片类型；一旦理解了决策树的工作原理，我们甚至也可以帮助人们判断需要佩戴的镜片类型。

　　隐形眼镜数据集①是非常著名的数据集，它包含很多患者眼部状况的观察条件以及医生推荐的
隐形眼镜类型。隐形眼镜类型包括硬材质、软材质以及不适合佩戴隐形眼镜。数据来源于UCI数据
库，简单修改过，将数据放入到项目中。

	fr=open("lenses.txt")
    lenses=[inst.strip().split('\t') for inst in fr.readlines()]
    labels=['age','prescript','astigmatic','tearrate']
    lensesTree=createTree(lenses,labels)
    treePlotter.createPlot(lensesTree)

**结果：**

![](assets/images/2017/09/v16dve7ug4j40o29g3qfptfp21.png)

　　
所示的决策树非常好地匹配了实验数据，然而这些匹配选项可能太多了。我们将这种问题称之为过度匹配（overfitting）。
为了减少过度匹配问题，我们可以裁剪决策树，去掉一些不
必要的叶子节点。如何剪裁？等学习了后面的章节再说。