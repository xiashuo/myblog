<div class="blog-article">
    <h1><a href="p.html?p=机器学习/机器学习之朴素贝叶斯" class="title">机器学习之朴素贝叶斯</a></h1>
    <span class="author">xiashuobad</span>
    <span class="time">2017-09-25 14:11</span>
    <span><a href="tags.html?t=机器学习" class="tag">机器学习</a></span>
    </div>
<br/>

## 基于贝叶斯决策理论的分类方法 ##
　　朴素贝叶斯是贝叶斯决策理论的一部分，所以学习朴素贝叶斯前，有必要了解一下贝叶斯决策理论。其实就是基于概率论的一种分类方法，利用概率分布来判断类别，即选择最高概率的决策：

- 如果p1(x, y) > p2(x, y)，那么属于类别1；
- 如果p2(x, y) > p1(x, y)，那么属于类别2。

　　但这两个准则并不是贝叶斯决策理论的所有内容。使用p1( )和p2( )只是为了尽可能简化
描述，而真正需要计算和比较的是p(c1|x, y)和p(c2|x, y)。这些符号所代表的具体意义是：
给定某个由x、 y表示的数据点，那么该数据点来自类别c1的概率是多少？数据点来自类别c2的概
率又是多少？具体地，应用贝叶斯准则得到：

　　　　　　　![](http://www.xsblog.club/assets/images/2017/09/pcs9ttnetsjmkruj62as09l8s0.png)

　　使用这些定义，可以定义贝叶斯分类准则为：

- 如果P(c1|x, y) > P(c2|x, y)，那么属于类别c1。
- 如果P(c1|x, y) < P(c2|x, y)，那么属于类别c2。

　　使用贝叶斯准则，可以通过已知的三个概率值来计算未知的概率值，接下来用上面的方法做个例子。
## 使用朴素贝叶斯进行文档分类 ##
　　在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词的出现或者不出现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。

　　要从文本中获取特征，需要先**拆分文本，将文本转换成数字向量**。以在线社区的留言板为例。为了不影响社区的发展，我们要屏蔽侮辱性的言论，所以要构建一个快速过滤器，如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。过滤这类内容是一个很常见的需求。对此问题建立两个类别：侮辱类和非侮辱类，使用1和0分别表示。
### 准备数据：从文本中构建词向量 ###
　　我们将把文本看成单词向量或者词条向量，也就是说将句子转换为向量。考虑出现在所有文档中的所有单词，再决定将哪些词纳入词汇表或者说所要的词汇集合，然后必须要将每一篇文档转换为词汇表上的向量。

**词表到向量的转换函数**

	def loadDataSet():
	    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
	                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
	                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
	                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
	                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
	                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
	    classVec = [0,1,0,1,0,1]    #1代表侮辱性文字，0代表正常言论
	    # 返回的第一个变量是进行词条切分后的文档集合，这些文档来自斑点犬爱好者留言板。
	    #返回的第二个变量是一个类别标签的集合。这里有两类，侮辱性和非侮辱性。
	    return postingList,classVec
	
	def createVocabList(dataSet):
	    vocabSet = set([])  #创建一个空集合
	    for document in dataSet:
	        vocabSet = vocabSet | set(document) #操作符|用于求两个集合的并集
	    return list(vocabSet)
	
	def setOfWords2Vec(vocabList, inputSet): #参数inputSet为词汇表及某个文档
	    returnVec = [0]*len(vocabList) #创建一个和词汇表等长的向量，并将其元素都设置为0
	    for word in inputSet:   #遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1
	        if word in vocabList:
	            returnVec[vocabList.index(word)] = 1
	        else: print("the word: %s is not in my Vocabulary!" % word)
	    return returnVec  #返回的是文档向量，向量的每一元素为1或0
**测试一下函数**

	listPost,listClasses=loadDataSet()
    myVocabList=createVocabList(listPost)
	print(myVocabList)
    print(setOfWords2Vec(myVocabList,listPost[0]))
    print(setOfWords2Vec(myVocabList,listPost[1]))
**结果：**

	['is', 'quit', 'how', 'posting', 'stop', 'cute', 'garbage', 'I', 'to', 'maybe', 'has', 'him', 'steak', 'flea', 'buying', 'dog', 'licks', 'help', 'mr', 'not', 'my', 'food', 'stupid', 'ate', 'so', 'problems', 'please', 'worthless', 'park', 'take', 'love', 'dalmation']
	[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]
	[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0]
　　该函数使用词汇表或者想要检查的所有单词作为输入，然后为其中每一个单词构建一个特征。一旦给定一篇文档（斑点犬网站上的一条留言），该文档就会被转换为词向量。接下来检查一下函数的有效性。myVocabList中索引为10的元素是"has"，该单词在第一篇文章中出现。
### 训练算法：从词向量计算概率 ###
　　现在已经知道一个词是否出现在一篇文档中，也知道该文档所属的类别。我们重写贝叶斯准则，将之前的x、 y 替换为w。粗体w表示这是一个向量，即它由多个数值组成。在这个例子中，数值个数与词汇表中的词个数相同。

　　　　　![](http://www.xsblog.club/assets/images/2017/09/vh3nrqe936g1uo0ssijuli60n7.png)

　　我们将使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。如何计算呢？首先可以通过类别i（侮辱性留言或非侮辱性留言）中文档数除以总的文档数来计算概率p(ci)。接下来计算p(w|ci)，这里就要用到朴素贝叶斯假设。如果将w展开为一个个独立特征，那么就可以将上述概率写作p(w0,w1,w2..wN|ci)。这里假设所有词都互相独立，该假设也称作条件独立性假设，它意味着可以使用p(w0|ci)p(w1|ci)p(w2|ci)...p(wN|ci)来计算上述概率，这就极大地简化了计算的过程。

**朴素贝叶斯分类器训练函数**

	def trainNB0(trainMatrix,trainCategory): #参数trainMatrix是转换成数字后的文档矩阵
	    numTrainDocs = len(trainMatrix) #文档的数量
	    numWords = len(trainMatrix[0]) #每一行的元素个数，这里就等于词汇表里单词的个数
	    pAbusive = sum(trainCategory)/float(numTrainDocs) #类别1的概率，即p(1)
	    p0Num = zeros(numWords); p1Num = zeros(numWords)
	    # p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones()
	    p0Denom = 0.0; p1Denom = 0.0
	    # p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0
	    for i in range(numTrainDocs):  #遍历所有文档
	        if trainCategory[i] == 1: # 如果该篇文档的类别为1
	            p1Num += trainMatrix[i] #则对应分类中，所有出现过的单词都加1
	            p1Denom += sum(trainMatrix[i]) #将该文档中单词的总数累加
	        else:  #如果是类别0，则相反执行相同的操作
	            p0Num += trainMatrix[i]
	            p0Denom += sum(trainMatrix[i])
	    #计算每个单词属于类别1的概率
	    p1Vect = p1Num/p1Denom          #change to log()
	    # p1Vect = log(p1Num/p1Denom)          #change to log()
	    #计算没个单词属于类别0的概率
	    p0Vect = p0Num/p0Denom          #change to log()
	    # p0Vect = log(p0Num/p0Denom)          #change to log()
	    return p0Vect,p1Vect,pAbusive
**测试一下该函数：**

	listPost,listClasses=loadDataSet()
    myVocabList=createVocabList(listPost)
	print(myVocabList)
	trainMat=[setOfWords2Vec(myVocabList,post) for post in listPost]
    p0v,p1v,pAb=trainNB0(trainMat,listClasses)
    print(pAb)
    print(p0v)
    print(p1v)

**结果：**

	['take', 'has', 'mr', 'stupid', 'my', 'not', 'so', 'ate', 'how', 'help', 'quit', 'garbage', 'to', 'food', 'problems', 'cute', 'dalmation', 'licks', 'park', 'love', 'worthless', 'buying', 'is', 'steak', 'dog', 'maybe', 'him', 'posting', 'flea', 'stop', 'I', 'please']
	0.5
	[ 0.          0.04166667  0.04166667  0.          0.125       0.
	  0.04166667  0.04166667  0.04166667  0.04166667  0.          0.
	  0.04166667  0.          0.04166667  0.04166667  0.04166667  0.04166667
	  0.          0.04166667  0.          0.          0.04166667  0.04166667
	  0.04166667  0.          0.08333333  0.          0.04166667  0.04166667
	  0.04166667  0.04166667]
	[ 0.05263158  0.          0.          0.15789474  0.          0.05263158
	  0.          0.          0.          0.          0.05263158  0.05263158
	  0.05263158  0.05263158  0.          0.          0.          0.
	  0.05263158  0.          0.10526316  0.05263158  0.          0.
	  0.10526316  0.05263158  0.05263158  0.05263158  0.          0.05263158
	  0.          0.        ]
　　首先，我们发现文档属于侮辱类的概率pAb为0.5，该值是正确的。接下来，看一看在给定文档类别条件下词汇表中单词的出现概率，看看是否正确。词汇表中的第一个词是"take"，其在类别1中出现1次，而在类别0中从未出现。对应的条件概率分别为0.05263158和0。该计算是正确的。我们找找所有概率中的最大值，该值出现在P(1)数组第3个下标位置，大小为0.15789474。在myVocabList的第3个下标位置上可以查到该单词是stupid。这意味着stupid是最能表征类别1（侮辱性文档类）的单词。
### 测试算法：根据现实情况修改分类器 ###
　　利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概
率，即计算p(w0|1)p(w1|1)p(w2|1)。如果其中一个概率值为0，那么最后的乘积也为0。为降低
这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2。
　　将trainNB0()的第4行和第5行修改为:

	p0Num = ones(numWords); p1Num = ones(numWords)
	p0Denom = 2.0; p1Denom = 2.0  
　　另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算乘积p(w0|ci)p(w1|ci)p(w2|ci)...p(wN|ci)时，由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。（用Python尝试相乘许多很小的数，最后四舍五入后会得到0。）在代数中有ln(a*b) = ln(a)+ln(b)，于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。通过修改return前的两行代码，将上述做法用到分类器中：

	p1Vect = log(p1Num/p1Denom) 
	p0Vect = log(p0Num/p0Denom)
　　现在已经准备好构建完整的分类器了。当使用NumPy向量处理功能时，这一切变得十分简单。

**朴素贝叶斯分类函数**

	def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): #参数vec2Classify是转换为数字的文档
	    # 这里用到了上面的贝叶斯公式，因为用log函数处理后，乘法变成了加法。
	    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #计算该文档属于类别1的概率
	    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1) #计算该文档属于类别0的概率
	    if p1 > p0: 
	        return 1
	    else:
	        return 0
	＃为了测试方便，写一个测试函数
	def testingNB():
	    listOPosts,listClasses = loadDataSet()
	    myVocabList = createVocabList(listOPosts)
	    print(myVocabList)
	    trainMat=[]
	    trainMat = [setOfWords2Vec(myVocabList, postinDoc) for postinDoc in listOPosts]
	    p0v,p1v,pAb = trainNB0(array(trainMat),array(listClasses))
	    testEntry = ['love', 'my', 'dalmation']
	    thisDoc = setOfWords2Vec(myVocabList, testEntry)
	    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0v, p1v, pAb))
	    testEntry = ['him', 'so', 'stupid']
	    thisDoc = setOfWords2Vec(myVocabList, testEntry)
	    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0v, p1v, pAb))
　　**运行testingNB函数，结果为：**

	['I', 'is', 'food', 'to', 'so', 'how', 'park', 'not', 'ate', 'buying', 'stupid', 'maybe', 'quit', 'him', 'worthless', 'stop', 'problems', 'garbage', 'steak', 'please', 'mr', 'my', 'flea', 'take', 'posting', 'help', 'cute', 'dalmation', 'has', 'love', 'licks', 'dog']
	['love', 'my', 'dalmation'] classified as:  0
	['him', 'so', 'stupid'] classified as:  1
　　这个例子非常简单，但是它展示了朴素贝叶斯分类器的工作原理。接下来，我们会对代码做些修改，使分类器工作得更好。
### 准备数据：文档词袋模型 ###
　　目前为止，我们将每个词的出现与否作为一个特征，这可以被描述为**词集模型**（set-of-words
model）。如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称为**词袋模型**（bag-of-words model）。在词袋中，每个单词可以出现多次，而在词集中，每个词只能出现一次。为适应词袋模型，需要对函数setOfWords2Vec()稍加修改，修改后的函数称为bagOfWords2Vec()。

	def bagOfWords2VecMN(vocabList, inputSet):
	    returnVec = [0]*len(vocabList)
	    for word in inputSet:
	        if word in vocabList:
	            returnVec[vocabList.index(word)] += 1 #与之前唯一不同是，每当遇到一个单词时，增加词向量中的对应值，而不只是将对应的数值设为1。
	    return returnVec
　　现在分类器已经构建好了，下面我们将利用该分类器来过滤垃圾邮件。
## 示例：使用朴素贝叶斯过滤垃圾邮件 ##
　　使用朴素贝叶斯解决一些现实生活中的问题时，需要先从文本内容得到字符串列表，然后生成词向量。下面这个例子中，我们将了解朴素贝叶斯的一个最著名的应用：电子邮件垃圾过滤。
### 准备数据：切分文本 ###

	def textParse(bigString):  # 输入参数为一串很长的字符串
	    import re
	    listOfTokens = re.split(r'\W*', bigString) #利用正则表达式，split函数进行切分，分隔符是除单词、数字外的任意字符串。
	    return [tok.lower() for tok in listOfTokens if len(tok) > 2] #全部转换为小写，并返回得到的一系列词组成的词表，只返回长度大于2的词
### 测试算法：使用朴素贝叶斯进行交叉验证 ###
	def spamTest():
	    docList = []; #创建所有邮件单词集合，初始为空
	    classList = [];#标签集合
	    for i in range(1, 26): #文件里一共有25封邮件，遍历25次
	        #这里读取文档时会存在编码问题，用如下方式解决
	        wordList = textParse(open('email/spam/%d.txt' % i,"rb").read().decode('GBK','ignore'))
	        docList.append(wordList) #将该篇邮件的词集加入到总词集中
	        classList.append(1) #添加标签1
	        wordList = textParse(open('email/ham/%d.txt' % i, "rb").read().decode('GBK', 'ignore'))
	        docList.append(wordList)  #将该篇邮件的词集加入到总词集中
	        classList.append(0) #添加标签0
	    vocabList = createVocabList(docList)  # 生成单词集合（去掉重复的）
	    trainingSet = list(range(50)); #创建一个大小为50的list
	    testSet = []  # 测试集
	    for i in range(10): #随机选择10篇邮件的向量词集作为测试样本
	        randIndex = int(random.uniform(0, len(trainingSet)))
	        testSet.append(trainingSet[randIndex])
	        del (trainingSet[randIndex]) #删除掉这10组，剩下的40组作为训练样本
	    trainMat = []; #训练集合矩阵
	    trainClasses = [] #训练标签集
	    for docIndex in trainingSet:  # train the classifier (get probs) trainNB0
	        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))#将每个训练样本单词向量加入到训练矩阵中
	        trainClasses.append(classList[docIndex]) #将每个训练样本的标签加入到训练标签集合
	    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses)) #计算属于对应标签的概率
	    errorCount = 0
	    #测试10组测试样本的预测结果，并打印错误率
	    for docIndex in testSet:  # classify the remaining items
	        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])
	        if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:
	            errorCount += 1
	            print("classification error", docList[docIndex])
	    error_rate=float(errorCount) / len(testSet)
	    print('the error rate is: ',error_rate)
	    return error_rate
**运行下面测试代码：**

	errorRate=0
    for i in range(10):
        errorRate +=spamTest()
    print("平均错误率为",float(errorRate/10))
**结果：**

	the error rate is:  0.0
	the error rate is:  0.0
	classification error ['home', 'based', 'business', 'opportunity', 'knocking', 'your', 'door', 'don抰', 'rude', 'and', 'let', 'this', 'chance', 'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts']
	classification error ['yeah', 'ready', 'may', 'not', 'here', 'because', 'jar', 'jar', 'has', 'plane', 'tickets', 'germany', 'for']
	the error rate is:  0.2
	classification error ['home', 'based', 'business', 'opportunity', 'knocking', 'your', 'door', 'don抰', 'rude', 'and', 'let', 'this', 'chance', 'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts']
	the error rate is:  0.1
	classification error ['experience', 'with', 'biggerpenis', 'today', 'grow', 'inches', 'more', 'the', 'safest', 'most', 'effective', 'methods', 'of_penisen1argement', 'save', 'your', 'time', 'and', 'money', 'bettererections', 'with', 'effective', 'ma1eenhancement', 'products', 'ma1eenhancement', 'supplement', 'trusted', 'millions', 'buy', 'today']
	the error rate is:  0.1
	the error rate is:  0.0
	classification error ['oem', 'adobe', 'microsoft', 'softwares', 'fast', 'order', 'and', 'download', 'microsoft', 'office', 'professional', 'plus', '2007', '2010', '129', 'microsoft', 'windows', 'ultimate', '119', 'adobe', 'photoshop', 'cs5', 'extended', 'adobe', 'acrobat', 'pro', 'extended', 'windows', 'professional', 'thousand', 'more', 'titles']
	the error rate is:  0.1
	classification error ['yeah', 'ready', 'may', 'not', 'here', 'because', 'jar', 'jar', 'has', 'plane', 'tickets', 'germany', 'for']
	classification error ['benoit', 'mandelbrot', '1924', '2010', 'benoit', 'mandelbrot', '1924', '2010', 'wilmott', 'team', 'benoit', 'mandelbrot', 'the', 'mathematician', 'the', 'father', 'fractal', 'mathematics', 'and', 'advocate', 'more', 'sophisticated', 'modelling', 'quantitative', 'finance', 'died', '14th', 'october', '2010', 'aged', 'wilmott', 'magazine', 'has', 'often', 'featured', 'mandelbrot', 'his', 'ideas', 'and', 'the', 'work', 'others', 'inspired', 'his', 'fundamental', 'insights', 'you', 'must', 'logged', 'view', 'these', 'articles', 'from', 'past', 'issues', 'wilmott', 'magazine']
	the error rate is:  0.2
	classification error ['scifinance', 'now', 'automatically', 'generates', 'gpu', 'enabled', 'pricing', 'risk', 'model', 'source', 'code', 'that', 'runs', '300x', 'faster', 'than', 'serial', 'code', 'using', 'new', 'nvidia', 'fermi', 'class', 'tesla', 'series', 'gpu', 'scifinance', 'derivatives', 'pricing', 'and', 'risk', 'model', 'development', 'tool', 'that', 'automatically', 'generates', 'and', 'gpu', 'enabled', 'source', 'code', 'from', 'concise', 'high', 'level', 'model', 'specifications', 'parallel', 'computing', 'cuda', 'programming', 'expertise', 'required', 'scifinance', 'automatic', 'gpu', 'enabled', 'monte', 'carlo', 'pricing', 'model', 'source', 'code', 'generation', 'capabilities', 'have', 'been', 'significantly', 'extended', 'the', 'latest', 'release', 'this', 'includes']
	the error rate is:  0.1
	classification error ['home', 'based', 'business', 'opportunity', 'knocking', 'your', 'door', 'don抰', 'rude', 'and', 'let', 'this', 'chance', 'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts']
	the error rate is:  0.1
	平均错误率为 0.09
　　通过10次分类测试，平均错误率为0.09，还凑合吧，随着学习的深入，后面会慢慢改进算法，关于朴素贝叶斯就到这里了。